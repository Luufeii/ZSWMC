{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里给四缺陷打上伪标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_file.M_attri import Att\n",
    "from py_file.Get_Data import DATA\n",
    "from py_file.data_set import MyDataSet\n",
    "from py_file.func_Test import Test_Func\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Resize(224)  # ResNet模型适合的图片大小为224x244\n",
    "# 输入的张量需要带着批次维度和通道维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备为：cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 定义训练的设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # 只有一张显卡的话，'cuda'和'cuda:0'是一样的\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'使用的设备为：{device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据和模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['original_wm', 'label_one_hot', 'denoise_wm', 'label_name']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/mnt/workspace/DATA/train_WM.npz'\n",
    "train_data = np.load(train_data_path)\n",
    "print(train_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_wm = train_data['original_wm']\n",
    "wm = train_data['denoise_wm']\n",
    "label = train_data['label_name']\n",
    "label_oh = train_data['label_one_hot']\n",
    "mul_ori_wm = []\n",
    "mul_wm = []\n",
    "mul_label = []\n",
    "mul_label_oh = []\n",
    "for i in range(len(label_oh)):\n",
    "    if label_oh[i].sum() >= 2:\n",
    "        mul_ori_wm.append(np.array(ori_wm[i]))\n",
    "        mul_wm.append(np.array(wm[i]))  # 元素是tensor的列表无法转换为张量，所以这里把列表里的元素转换为np.array\n",
    "        mul_label.append(label[i])\n",
    "        mul_label_oh.append(label_oh[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20716/1119019278.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  mul_wm_tensor = trans(torch.tensor(mul_wm, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20300, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "mul_wm_tensor = trans(torch.tensor(mul_wm, dtype=torch.float32))\n",
    "mul_wm_tensor = torch.reshape(mul_wm_tensor,(-1,1,224,224))  # 给图片加上通道\n",
    "print(mul_wm_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取类别名称和多热编码对应的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C+EL' 'C+EL+L' 'C+EL+L+S' 'C+EL+S' 'C+ER' 'C+ER+L' 'C+ER+L+S' 'C+ER+S'\n",
      " 'C+L' 'C+L+S' 'C+S' 'D+EL' 'D+EL+L' 'D+EL+L+S' 'D+EL+S' 'D+ER' 'D+ER+L'\n",
      " 'D+ER+L+S' 'D+ER+S' 'D+L' 'D+L+S' 'D+S' 'EL+L' 'EL+L+S' 'EL+S' 'ER+L'\n",
      " 'ER+L+S' 'ER+S' 'L+S']\n",
      "[  700  3500  2800     0  2100  4900  4200  1400  6300  5600  7000  8400\n",
      " 11200 10500  7700  9800 12600 11900  9100 14000 13300 14700 16100 15400\n",
      " 18900 17500 16800 19600 18200]\n",
      "[[0 0 0 0 1 0 1 0]\n",
      " [0 0 0 1 0 0 1 0]\n",
      " [0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 1 0]\n",
      " [0 1 0 0 0 0 1 0]\n",
      " [0 1 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 1 0]\n",
      " [0 1 0 1 0 0 0 0]\n",
      " [0 1 0 1 0 0 1 0]\n",
      " [0 1 0 1 1 0 0 0]\n",
      " [0 1 0 1 1 0 1 0]\n",
      " [0 1 1 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 0 0]\n",
      " [0 1 1 0 1 0 1 0]\n",
      " [1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 0]\n",
      " [1 0 0 1 0 0 1 0]\n",
      " [1 0 0 1 1 0 0 0]\n",
      " [1 0 0 1 1 0 1 0]\n",
      " [1 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 1 0]\n",
      " [1 0 1 0 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0]]\n",
      "[18200 19600 17500 16800 18900 16100 15400 14700 14000 13300  9800  9100\n",
      " 12600 11900  8400  7700 11200 10500  7000  6300  5600  2100  1400  4900\n",
      "  4200   700     0  3500  2800]\n"
     ]
    }
   ],
   "source": [
    "mul_label_unique, label_indices = np.unique(mul_label, return_index=True)\n",
    "print(mul_label_unique)\n",
    "print(label_indices)\n",
    "# print(two_label_unique[indices])\n",
    "mul_label_oh_unique ,label_oh_indices = np.unique(mul_label_oh,axis=0, return_index=True)\n",
    "print(mul_label_oh_unique)\n",
    "print(label_oh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C+EL': array([1, 0, 1, 0, 0, 0, 0, 0], dtype=int32), 'C+EL+L': array([1, 0, 1, 0, 1, 0, 0, 0], dtype=int32), 'C+EL+L+S': array([1, 0, 1, 0, 1, 0, 1, 0], dtype=int32), 'C+EL+S': array([1, 0, 1, 0, 0, 0, 1, 0], dtype=int32), 'C+ER': array([1, 0, 0, 1, 0, 0, 0, 0], dtype=int32), 'C+ER+L': array([1, 0, 0, 1, 1, 0, 0, 0], dtype=int32), 'C+ER+L+S': array([1, 0, 0, 1, 1, 0, 1, 0], dtype=int32), 'C+ER+S': array([1, 0, 0, 1, 0, 0, 1, 0], dtype=int32), 'C+L': array([1, 0, 0, 0, 1, 0, 0, 0], dtype=int32), 'C+L+S': array([1, 0, 0, 0, 1, 0, 1, 0], dtype=int32), 'C+S': array([1, 0, 0, 0, 0, 0, 1, 0], dtype=int32), 'D+EL': array([0, 1, 1, 0, 0, 0, 0, 0], dtype=int32), 'D+EL+L': array([0, 1, 1, 0, 1, 0, 0, 0], dtype=int32), 'D+EL+L+S': array([0, 1, 1, 0, 1, 0, 1, 0], dtype=int32), 'D+EL+S': array([0, 1, 1, 0, 0, 0, 1, 0], dtype=int32), 'D+ER': array([0, 1, 0, 1, 0, 0, 0, 0], dtype=int32), 'D+ER+L': array([0, 1, 0, 1, 1, 0, 0, 0], dtype=int32), 'D+ER+L+S': array([0, 1, 0, 1, 1, 0, 1, 0], dtype=int32), 'D+ER+S': array([0, 1, 0, 1, 0, 0, 1, 0], dtype=int32), 'D+L': array([0, 1, 0, 0, 1, 0, 0, 0], dtype=int32), 'D+L+S': array([0, 1, 0, 0, 1, 0, 1, 0], dtype=int32), 'D+S': array([0, 1, 0, 0, 0, 0, 1, 0], dtype=int32), 'EL+L': array([0, 0, 1, 0, 1, 0, 0, 0], dtype=int32), 'EL+L+S': array([0, 0, 1, 0, 1, 0, 1, 0], dtype=int32), 'EL+S': array([0, 0, 1, 0, 0, 0, 1, 0], dtype=int32), 'ER+L': array([0, 0, 0, 1, 1, 0, 0, 0], dtype=int32), 'ER+L+S': array([0, 0, 0, 1, 1, 0, 1, 0], dtype=int32), 'ER+S': array([0, 0, 0, 1, 0, 0, 1, 0], dtype=int32), 'L+S': array([0, 0, 0, 0, 1, 0, 1, 0], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "oh_dict = {}\n",
    "for index in label_indices:\n",
    "    oh_dict[mul_label[index]] = mul_label_oh[index]\n",
    "print(oh_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型和语义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = torch.load('model_saved_pseudo/train_single_two_three.pth')\n",
    "model.eval()\n",
    "\n",
    "with open('updated_semantic_1_2_3_4/updated_single_dict.pkl', 'rb') as file:\n",
    "    single_defect_att = pickle.load(file)\n",
    "with open('updated_semantic_1_2_3_4/updated_mul_dict.pkl', 'rb') as file:\n",
    "    mul_defect_att = pickle.load(file)\n",
    "\n",
    "from collections import Counter\n",
    "# 加载二、三、四缺陷的字典\n",
    "two_defect_att = {}\n",
    "three_defect_att = {}\n",
    "four_defect_att = {}\n",
    "for label in mul_defect_att.keys():\n",
    "    count = Counter(label)\n",
    "    if count['+'] == 1:\n",
    "        two_defect_att[label] = mul_defect_att[label]\n",
    "    if count['+'] == 2:\n",
    "        three_defect_att[label] = mul_defect_att[label]\n",
    "    if count['+'] == 3:\n",
    "        four_defect_att[label] = mul_defect_att[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_dataset = MyDataSet(mul_wm_tensor, mul_label)\n",
    "mul_loader = DataLoader(mul_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义预测标签的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):  # 参数v1,v2是np.array,不能是tensor，可以用np.array()将tensor转换为array\n",
    "    # 计算两个向量的点积\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    # 计算两个向量的模\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    # 计算余弦相似度\n",
    "    similarity = dot_product / (norm_v1 * norm_v2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给一个属性张量判断标签\n",
    "def max_similarity(att_tensor, defe_dict, t = -1):  # 使用余弦相似度\n",
    "    max_sim = -2\n",
    "    att_array = np.array(att_tensor.cpu())  # 在GPU上的张量无法转换为array，只有在CPU上的张量才能转换为array\n",
    "    for label,attribute in defe_dict.items():\n",
    "        attribute_array = np.array(attribute)\n",
    "        simi = cosine_similarity(att_array,attribute_array)\n",
    "\n",
    "        if simi>max_sim:\n",
    "            max_sim = simi\n",
    "            predict_label = label\n",
    "    \n",
    "    if t!=-1 and max_sim<t:\n",
    "        predict_label = 'unknown'\n",
    "    \n",
    "    return predict_label  # 这是一个str类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从一个批次的输出属性中，获取预测的标签\n",
    "def Predict_label(outputs, defe_att_dic, t = -1):\n",
    "        outputs_labels = []   # tensor只能将元素为数字类型的列表转换为tensor\n",
    "        for out in outputs:\n",
    "            outputs_labels.append(max_similarity(out,defe_att_dic,t))\n",
    "        return outputs_labels\n",
    "\n",
    "\n",
    "# 从dataloader中提取label\n",
    "def get_label(model, dataloader, att_dic, t = -1):\n",
    "    label_true = []\n",
    "    label_pre = []\n",
    "    with torch.no_grad(): \n",
    "        for data in dataloader:  \n",
    "            imgs,labels = data\n",
    "\n",
    "            for label in labels:\n",
    "                label_true.append(label)\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "\n",
    "            y_pre = Predict_label(outputs, att_dic, t)\n",
    "            label_pre = label_pre + y_pre\n",
    "\n",
    "    return label_true,label_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(ori_wm, denoised_wm, real_label, pred_label):\n",
    "    cleaned_ori_wm = []\n",
    "    cleaned_denoised_wm = []\n",
    "    cleaned_real_label = []\n",
    "    cleaned_pred_label = []\n",
    "\n",
    "    for i in range(len(pred_label)):\n",
    "        if pred_label[i] != 'unknown':\n",
    "            cleaned_ori_wm.append(ori_wm[i])\n",
    "            cleaned_denoised_wm.append(denoised_wm[i])\n",
    "            cleaned_real_label.append(real_label[i])\n",
    "            cleaned_pred_label.append(pred_label[i])\n",
    "    \n",
    "    return cleaned_ori_wm, cleaned_denoised_wm, cleaned_real_label, cleaned_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取伪标签的粗粒度准确率和细粒度准确率\n",
    "def get_acc(real_label, pred_label, label_unique):\n",
    "    Coarse_grained_num = 0\n",
    "    for it in real_label:\n",
    "        if it in label_unique:\n",
    "            Coarse_grained_num += 1\n",
    "    Coarse_grained_acc = Coarse_grained_num / len(real_label)\n",
    "\n",
    "    real_label_array = np.array(real_label)\n",
    "    pred_label_array = np.array(pred_label)\n",
    "    Fine_grained_acc = np.sum(real_label_array == pred_label_array) / len(real_label)\n",
    "\n",
    "    return Coarse_grained_acc, Fine_grained_acc, len(real_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取被判断为四故障的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_real_label, mul_predict_label = get_label(model, mul_loader, mul_defect_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "dict_keys(['C+EL+L+S', 'C+ER+L+S', 'D+EL+L+S', 'D+ER+L+S'])\n"
     ]
    }
   ],
   "source": [
    "four_label_unique = four_defect_att.keys()\n",
    "print(len(four_label_unique))\n",
    "print(four_label_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_four_wm = []\n",
    "predicted_four_ori_wm = []\n",
    "predicted_four_real_label = []\n",
    "for i in range(len(mul_predict_label)):\n",
    "    if mul_predict_label[i] in four_label_unique:\n",
    "        predicted_four_wm.append(mul_wm[i])\n",
    "        predicted_four_ori_wm.append(mul_ori_wm[i])\n",
    "        predicted_four_real_label.append(mul_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372 1673\n",
      "0.22235505080693366\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in range(len(predicted_four_wm)):\n",
    "    if predicted_four_real_label[i] not in four_label_unique:\n",
    "        n = n + 1\n",
    "print(n, len(predicted_four_wm))\n",
    "print(n/len(predicted_four_wm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1673, 1, 224, 224]) (1673,)\n"
     ]
    }
   ],
   "source": [
    "predicted_four_wm_tensor = trans(torch.tensor(np.array(predicted_four_wm), dtype=torch.float32))\n",
    "predicted_four_wm_tensor = torch.reshape(predicted_four_wm_tensor,(-1,1,224,224))\n",
    "\n",
    "predicted_four_real_label = np.array(predicted_four_real_label)\n",
    "print(predicted_four_wm_tensor.shape, predicted_four_real_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_four_dataset = MyDataSet(predicted_four_wm_tensor, predicted_four_real_label)\n",
    "predicted_four_loader = DataLoader(predicted_four_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试验参数T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse_acc: 0.7820748925721301, fine_acc: 0.583793738489871, num: 1629\n"
     ]
    }
   ],
   "source": [
    "real_label, predict_label = get_label(model, predicted_four_loader, four_defect_att, T)\n",
    "saved_ori_wm, saved_wm, saved_real_label, saved_predict_label = clean_data(predicted_four_ori_wm, predicted_four_wm,\n",
    " real_label, predict_label)\n",
    "\n",
    "label_unique = list(four_defect_att.keys())\n",
    "coarse_acc, fine_acc, num = get_acc(saved_real_label, saved_predict_label, label_unique)\n",
    "\n",
    "print(f\"coarse_acc: {coarse_acc}, fine_acc: {fine_acc}, num: {num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存打了四故障伪标签的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1629, 8)\n"
     ]
    }
   ],
   "source": [
    "saved_label_oh = []\n",
    "for l in saved_predict_label:\n",
    "    saved_label_oh.append(oh_dict[l])\n",
    "    \n",
    "saved_label_oh = np.array(saved_label_oh)\n",
    "print(saved_label_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1629, 52, 52)\n",
      "(1629, 52, 52)\n",
      "(1629,)\n"
     ]
    }
   ],
   "source": [
    "saved_ori_wm = np.array(saved_ori_wm)\n",
    "saved_wm = np.array(saved_wm)\n",
    "saved_predict_label = np.array(saved_predict_label)\n",
    "print(saved_ori_wm.shape)\n",
    "print(saved_wm.shape)\n",
    "print(saved_predict_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data_fake_label/four_fake_label_WM.npz', original_wm = saved_ori_wm, label_one_hot = saved_label_oh,\n",
    " denoise_wm = saved_wm, label_name = saved_predict_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
